{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce3b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "#from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from perplexity import Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a1dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)  # Loads the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "#anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "#deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "#groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "perplexity_api_key = os.getenv('PERPLEXITY_API_KEY')\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if perplexity_api_key:\n",
    "    print(f\"Perplexity API Key exists and begins {perplexity_api_key[:4]}\") \n",
    "else:\n",
    "    print(\"Perplexity API Key not set (and this is optional)\")\n",
    "# --- IGNORE ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = (\n",
    "    \"Please come up with a challenging, nuanced question that I can ask \"\n",
    "    \"a number of LLMs to evaluate their intelligence. \"\n",
    "    \"Answer only with the question, no explanation , no table.\"\n",
    ")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd11215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Perplexity_client = Perplexity(api_key=os.environ[\"PERPLEXITY_API_KEY\"])\n",
    "\n",
    "# ‚úÖ Use the chat completion endpoint for AI-generated answers\n",
    "response = Perplexity_client.chat.completions.create(\n",
    "    model=\"sonar-pro\",  # or sonar-small if you want cheaper/faster\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "question =  response.choices[0].message.content\n",
    "\n",
    "print(\"Generated Question:\")\n",
    "display(Markdown(f\"**{question}**\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e948c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "gemini_client = genai.Client(api_key = os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "# Gemini expects content as a list of parts (e.g., [question])\n",
    "answer = gemini_client.models.generate_content(\n",
    "    model=model_name, contents=[question])\n",
    "\n",
    "display(Markdown(answer.text))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2652f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Perplexity_client = Perplexity(api_key=os.environ[\"PERPLEXITY_API_KEY\"])\n",
    "model_name = \"sonar-pro\" # or sonar-small if you want cheaper/faster\n",
    "# ‚úÖ Use the chat completion endpoint for AI-generated answers\n",
    "response = Perplexity_client.chat.completions.create(\n",
    "    model=model_name,  \n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer =  response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee169d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "api_key = os.getenv(\"OLLAMA_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"‚ö†Ô∏è OLLAMA_API_KEY not found. Check your .env file.\")\n",
    "\n",
    "# Initialize Ollama client\n",
    "client = Client(\n",
    "    host=\"https://ollama.com\",\n",
    "    headers={'Authorization': f'Bearer {api_key}'}\n",
    ")\n",
    "\n",
    "# --- Your arrays already exist ---\n",
    "# competitors = [\"model1\", \"model2\", \"model3\"]\n",
    "# answers = [\"answer from model1\", \"answer from model2\", \"answer from model3\"]\n",
    "\n",
    "# --- Build the judging prompt dynamically ---\n",
    "judge_prompt = (\n",
    "    \"You are a neutral, strict evaluator. Below are answers from different models \"\n",
    "    \"to the same question. Rank them from best to worst based on:\\n\"\n",
    "    \"- Accuracy and factual correctness\\n\"\n",
    "    \"- Clarity and coherence\\n\"\n",
    "    \"- Depth of reasoning and completeness\\n\\n\"\n",
    ")\n",
    "\n",
    "for i, (model, answer) in enumerate(zip(competitors, answers), 1):\n",
    "    judge_prompt += f\"Model {i} ({model}):\\n{answer}\\n\\n\"\n",
    "\n",
    "judge_prompt += (\n",
    "    \"Now give your ranking and reasoning in this format:\\n\"\n",
    "    \"1. model_name - reason\\n\"\n",
    "    \"2. model_name - reason\\n\"\n",
    "    \"3. model_name - reason\\n\"\n",
    ")\n",
    "\n",
    "# --- Judge using Ollama ---\n",
    "print(\"\\nüß† Evaluating models...\\n\")\n",
    "\n",
    "response_text = \"\"\n",
    "for part in client.chat(\"gpt-oss:120b\", messages=[{'role': 'user', 'content': judge_prompt}], stream=True):\n",
    "    content = part[\"message\"][\"content\"]\n",
    "    print(content, end=\"\", flush=True)\n",
    "    response_text += content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineer-course (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
